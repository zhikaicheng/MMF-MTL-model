import os
import io
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.regularizers import l2
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dropout
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from osgeo import gdal
import cv2
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import StandardScaler

# 设置模型参数
batch_size = 160
epochs = 100

# 加载ResNet50模型，去掉顶层
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(60, 60, 3))

# 冻结ResNet50的前几层，让后面的层能够针对当前任务进行微调
from tensorflow.keras.initializers import HeNormal
for layer in resnet_base.layers:
    if hasattr(layer, 'kernel_initializer'):
        layer.kernel_initializer = HeNormal()
for layer in resnet_base.layers[:10]:
    layer.trainable = False

for layer in resnet_base.layers[10:]:
    layer.trainable = True

# 在ResNet50的顶层增加自定义的全局平均池化层和全连接层
from tensorflow.keras.regularizers import l2
x = resnet_base.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu',kernel_regularizer=l2(0.6))(x)
x = Dense(512, activation='relu',kernel_regularizer=l2(0.6))(x)
x = Dense(128, activation='relu',kernel_regularizer=l2(0.6))(x)
predictions = Dense(1)(x)
# 定义模型
model = Model(inputs=resnet_base.input, outputs=predictions)

initial_learning_rate = 0.0001 #LAI 0.0001
warmup_epochs = 3
total_epochs = 100
steps_per_epoch = len(X_train) / batch_size
warmup_steps = warmup_epochs * steps_per_epoch

def cosine_decay_with_warmup(epoch, lr):
    if epoch < warmup_epochs:
        return (initial_learning_rate / warmup_epochs) * (epoch+1)
    else:
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        lr = 0.5 * initial_learning_rate * (1 + np.cos(progress * np.pi))
        return lr


optimizer = tf.keras.optimizers.Adam(learning_rate=tf.Variable(initial_learning_rate))
lr_schedule = tf.keras.callbacks.LearningRateScheduler(cosine_decay_with_warmup)

model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])
early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)

history = model.fit(
    X_train, y_train[:, 1],
    epochs=total_epochs,
    batch_size=batch_size,
    validation_data=(X_test, y_test[:, 1]),
    callbacks=[early_stopping, lr_schedule]
)
# 在测试集上评估模型
results = model.evaluate(X_test, y_test[:, 1])
print(results)

#根据模型loss曲线判断epochs
import matplotlib.pyplot as plt

# 绘制训练集和测试集的loss结果曲线
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
